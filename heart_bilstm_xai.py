# -*- coding: utf-8 -*-
"""Heart_BiLSTM_XAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_9nHcrt3jQnbISmZEhjFnew0hGqMBzyF

# **`Patient-wise splitting of PTB-XL dataset`**
"""

pip install wfdb torchsummary torchinfo

"""Import necessary packages"""

import os
import ast
import wfdb

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow.keras as keras

sns.set_style('darkgrid')

from zipfile import ZipFile

# specifying the zip file name 
#### PTB-XL v 1.0.1
file_name = "/content/drive/MyDrive/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip"
#### PTB-XL v 1.0.3
file_name = "/content/drive/MyDrive/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip"
# opening the zip file in READ mode
with ZipFile(file_name, 'r') as zip:
    # printing all the contents of the zip file
    zip.printdir()

    # extracting all the files
    print('Extracting all the files now...')
    zip.extractall()
    print('Done!')

from google.colab import drive
drive.mount('/content/drive')

# ============================
# Bi-directional LSTM for PTB-XL (5 superclasses)
# ============================

import os
import wfdb
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from scipy.signal import butter, filtfilt, iirnotch
from sklearn.model_selection import train_test_split

# --------------------------------------------------------
# 1. Filtering & Preprocessing
# --------------------------------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def bandpass_filter(signal, lowcut=0.5, highcut=40, fs=100, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, signal)

def notch_filter(signal, notch_freq=50, fs=100, Q=30):
    b, a = iirnotch(notch_freq, Q, fs)
    return filtfilt(b, a, signal)

def zscore_normalize(signal):
    mean = np.mean(signal)
    std = np.std(signal)
    return (signal - mean) / (std + 1e-8)

def preprocess_ecg(ecg_data, fs=100, target_len=1000, apply_notch=False):
    processed = []
    for lead in range(ecg_data.shape[0]):
        sig = ecg_data[lead]
        sig = bandpass_filter(sig, 0.5, 40, fs=fs)
        if apply_notch:
            sig = notch_filter(sig, notch_freq=50, fs=fs)
        sig = zscore_normalize(sig)
        if len(sig) > target_len:
            sig = sig[:target_len]
        elif len(sig) < target_len:
            sig = np.pad(sig, (0, target_len - len(sig)), 'constant')
        processed.append(sig)
    return np.array(processed)

# --------------------------------------------------------
# 2. Label Mapping (5 Superclasses)
# --------------------------------------------------------
def load_scored_labels(base_path):
    return pd.read_csv(os.path.join(base_path, "/content/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/scp_statements.csv"), index_col=0)

def map_superclass(scp_codes_str, statements, super_classes):
    # Convert string dict -> dict
    scp_codes = eval(scp_codes_str)
    mapped = []
    for code in scp_codes.keys():
        if code in statements.index:
            sup = statements.loc[code].diagnostic_class
            if sup in super_classes:
                mapped.append(sup)
    if len(mapped) == 0:
        return None
    return mapped[0]  # take first main superclass

# --------------------------------------------------------
# 3. Load PTB-XL with patient-wise split
# --------------------------------------------------------
def load_ptbxl_patientwise(base_path, sampling_rate=100, target_len=1000, apply_notch=False):
    df = pd.read_csv(os.path.join(base_path, "/content/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/ptbxl_database.csv"))
    statements = load_scored_labels(base_path)
    super_classes = ['NORM', 'MI', 'STTC', 'HYP', 'CD']

    # Choose correct filenames
    if sampling_rate == 100:
        df['filename'] = df['filename_lr'].apply(lambda x: os.path.join(base_path, x))
    else:
        df['filename'] = df['filename_hr'].apply(lambda x: os.path.join(base_path, x))

    # Map to superclasses
    df['superclass'] = df['scp_codes'].apply(lambda x: map_superclass(x, statements, super_classes))
    df = df[df['superclass'].notnull()]
    print(df['superclass'].value_counts())
    # Map to integers
    label_map = {sc: i for i, sc in enumerate(super_classes)}
    df['label'] = df['superclass'].map(label_map)

    # Patient-wise split
    patients = df['patient_id'].unique()
    train_pat, test_pat = train_test_split(patients, test_size=0.30, random_state=42)
    val_pat, test_pat = train_test_split(test_pat, test_size=0.50, random_state=42)

    split_map = {}
    for p in train_pat: split_map[p] = "train"
    for p in val_pat: split_map[p] = "val"
    for p in test_pat: split_map[p] = "test"

    df['split'] = df['patient_id'].map(split_map)

    datasets = {"train": [], "val": [], "test": []}
    labels = {"train": [], "val": [], "test": []}

    for idx, row in df.iterrows():
        record = wfdb.rdsamp(row['filename'])
        ecg_data = record[0].T
        processed = preprocess_ecg(ecg_data, fs=sampling_rate, target_len=target_len, apply_notch=apply_notch)
        datasets[row['split']].append(processed)
        labels[row['split']].append(row['label'])

    return datasets, labels, df, label_map

# --------------------------------------------------------
# 4. Torch Dataset Wrapper
# --------------------------------------------------------
class ECGDataset(Dataset):
    def __init__(self, signals, labels):
        self.signals = torch.tensor(np.array(signals), dtype=torch.float32)
        self.labels = torch.tensor(np.array(labels), dtype=torch.long)
    def __len__(self):
        return len(self.labels)
    def __getitem__(self, idx):
        return self.signals[idx], self.labels[idx]

# --------------------------------------------------------
# 5. Bi-LSTM Model
# --------------------------------------------------------
class BiLSTMClassifier(nn.Module):
    def __init__(self, input_size=12, hidden_size=128, num_layers=2, num_classes=5, dropout=0.5):
        super(BiLSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        # x: (batch, channels, seq_len) -> (batch, seq_len, channels)
        x = x.permute(0, 2, 1)
        out, _ = self.lstm(x)
        out = out[:, -1, :]  # last timestep
        out = self.fc(out)
        return out

# --------------------------------------------------------
# 6. Training Loop
# --------------------------------------------------------
def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device="cuda"):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    model.to(device)
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct, total = 0, 0
        for signals, labels in train_loader:
            signals, labels = signals.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(signals)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        train_acc = correct / total
        train_accuracies.append(train_acc)
        # Validation
        model.eval()
        val_loss = 0.0
        correct, total = 0, 0
        with torch.no_grad():
            for signals, labels in val_loader:
                signals, labels = signals.to(device), labels.to(device)
                outputs = model(signals)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_acc = correct / total
        val_losses.append(avg_val_loss)
        val_accuracies.append(val_acc)

        print(f"Epoch [{epoch+1}/{num_epochs}], "
              f"Train Acc: {train_acc:.4f}, "
              f"Train Loss: {avg_train_loss:.4f}, "
              f"Val Loss: {avg_val_loss:.4f}, "
              f"Val Acc: {val_acc:.4f}")

    return train_acc, val_acc,train_losses, val_losses

# --------------------------------------------------------
# 7. Run Training
# --------------------------------------------------------
base_path = "/content/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/"  # <-- update this
datasets, labels, df, label_map = load_ptbxl_patientwise(base_path, sampling_rate=100)

train_dataset = ECGDataset(datasets["train"], labels["train"])
val_dataset   = ECGDataset(datasets["val"], labels["val"])
test_dataset  = ECGDataset(datasets["test"], labels["test"])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)

#model = BiLSTMClassifier(num_classes=len(label_map))
#train_accuracies, val_accuracies, train_losses, val_losses = train_model(model, train_loader, val_loader, num_epochs=20, lr=1e-3, device="cuda" if torch.cuda.is_available() else "cpu")

print("Number of training samples:", len(train_dataset))
print("Number of validation samples:", len(val_dataset))
print("Number of testing samples:", len(test_dataset))

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.metrics import accuracy_score
import numpy as np


# ===================== BiLSTM Model =====================
class BiLSTMClassifier(nn.Module):
    def __init__(self, input_size=12, hidden_size=128, num_layers=2, num_classes=5, dropout=0.5):
        super(BiLSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x, return_probs=False):
        # x: (batch, channels, seq_len) -> (batch, seq_len, channels)
        x = x.permute(0, 2, 1)
        out, _ = self.lstm(x)
        out = self.dropout(out[:, -1, :])  # last timestep
        logits = self.fc(out)

        if return_probs:
            return logits, F.softmax(logits, dim=1)
        return logits


# ===================== Early Stopping =====================
class EarlyStopping:
    def __init__(self, patience=5, delta=0):
        self.patience = patience
        self.delta = delta
        self.best_score = None
        self.counter = 0
        self.early_stop = False
        self.best_state = None

    def __call__(self, val_score, model):
        if self.best_score is None:
            self.best_score = val_score
            self.best_state = model.state_dict()
        elif val_score < self.best_score + self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = val_score
            self.best_state = model.state_dict()
            self.counter = 0

def train_model(model, train_loader, val_loader, num_epochs=50, lr=1e-3, patience=5, device="cuda"):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    early_stopping = EarlyStopping(patience=patience)  # will track best val accuracy
    model.to(device)

    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []

    for epoch in range(num_epochs):
        # ---- Training ----
        model.train()
        train_loss = 0
        correct, total = 0, 0
        for X, y in train_loader:
            X, y = X.to(device), y.to(device)

            optimizer.zero_grad()
            logits = model(X)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * X.size(0)
            _, predicted = torch.max(logits, 1)
            total += y.size(0)
            correct += (predicted == y).sum().item()


        avg_train_loss = train_loss/len(train_loader.dataset)
        train_losses.append(avg_train_loss)
        train_acc = correct / total
        train_accuracies.append(train_acc)

        # ---- Validation ----
        model.eval()
        val_loss = 0
        y_true, y_pred, y_probs = [], [], []

        with torch.no_grad():
            for X, y in val_loader:
                X, y = X.to(device), y.to(device)
                logits, probs = model(X, return_probs=True)
                loss = criterion(logits, y)

                val_loss += loss.item() * X.size(0)
                y_true.extend(y.cpu().numpy())
                y_pred.extend(torch.argmax(probs, 1).cpu().numpy())
                y_probs.extend(probs.cpu().numpy())

        # Metrics
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        acc = accuracy_score(y_true, y_pred)

        avg_val_loss = val_loss/len(val_loader.dataset)
        val_losses.append(avg_val_loss)
        val_accuracies.append(acc)


        print(f"Epoch {epoch+1}/{num_epochs} "
              f"- Train Loss: {avg_train_loss:.4f} "
              f"- Val Loss: {avg_val_loss:.4f} "
              f"- Val Acc: {acc:.4f}")

        # ---- Early Stopping on Val Accuracy ----
        early_stopping(acc, model)
        if early_stopping.early_stop:
            print("Early stopping triggered (on validation accuracy)!")
            break

    # Load best model
    model.load_state_dict(early_stopping.best_state)
    return model, train_accuracies, val_accuracies, train_losses, val_losses

from torchinfo import summary
import torch

def print_model_summary(model, seq_len, input_size=12, batch_size=1):
    """
    Prints a clean summary of a PyTorch model (BiLSTM/CNN/CNN+BiLSTM) for ECG input.

    Parameters:
    - model: PyTorch nn.Module
    - seq_len: sequence length (number of timesteps)
    - input_size: number of features per timestep (e.g., ECG leads)
    - batch_size: dummy batch size for summary
    """
    # Create a dummy input tensor
    dummy_input = torch.randn(batch_size, seq_len, input_size).float()

    # Pass dummy input via input_data to torchinfo
    print("===== Model Summary =====")
    summary(model, input_data=dummy_input, col_names=("input_size", "output_size", "num_params", "trainable"))

    # Print total and trainable parameters manually
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nTotal parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print("=========================")
model = BiLSTMClassifier(num_classes=len(label_map))
print_model_summary(model, seq_len=1000, input_size=12, batch_size=1)

model = BiLSTMClassifier(num_classes=len(label_map))
model, train_accuracies, val_accuracies, train_losses, val_losses = train_model(model, train_loader, val_loader, num_epochs=8, lr=1e-2, device="cuda" if torch.cuda.is_available() else "cpu")

# Loss curve
plt.figure(figsize=(10,4))
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Val Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.show()

# Accuracy curve
plt.figure(figsize=(10,4))
plt.plot(train_accuracies, label="Train Accuracy")
plt.plot(val_accuracies, label="Val Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Training vs Validation Accuracy")
plt.legend()
plt.show()

# After training and testing
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
top_classes = df['superclass'].value_counts().index[:5]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.eval()
y_true, y_pred = [], []
with torch.no_grad():
    for signals, labels in test_loader:
        signals, labels = signals.to(device), labels.to(device)
        outputs = model(signals)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

print(classification_report(y_true, y_pred, target_names=top_classes))
class_names = ["NORM", "MI", "STTC", "CD", "HYP"]
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True,linewidth=.5,fmt="d")
plt.title('Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ==============================
# CNN Model
# ==============================
class ECG_CNN(nn.Module):
    def __init__(self, input_channels=12, num_classes=5):
        super(ECG_CNN, self).__init__()

        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=7, stride=1, padding=3)
        self.bn1 = nn.BatchNorm1d(32)
        self.pool1 = nn.MaxPool1d(2)

        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)
        self.bn2 = nn.BatchNorm1d(64)
        self.pool2 = nn.MaxPool1d(2)

        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm1d(128)
        self.pool3 = nn.MaxPool1d(2)

        self.global_pool = nn.AdaptiveAvgPool1d(1)   # compress time dimension
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x):
        # x shape: (batch, channels=12, timesteps)
        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))
        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))
        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))
        x = self.global_pool(x).squeeze(-1)   # (batch, features)
        x = self.fc(x)
        return x

# ==============================
# Training Function
# ==============================
def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20, device="cuda"):
    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}
    model.to(device)

    for epoch in range(epochs):
        # ---- Training ----
        model.train()
        train_loss, correct, total = 0, 0, 0

        for X, y in train_loader:
            X, y = X.to(device), y.to(device)

            optimizer.zero_grad()
            outputs = model(X)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == y).sum().item()
            total += y.size(0)

        train_acc = correct / total

        # ---- Validation ----
        model.eval()
        val_loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for X, y in val_loader:
                X, y = X.to(device), y.to(device)
                outputs = model(X)
                loss = criterion(outputs, y)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == y).sum().item()
                total += y.size(0)

        val_acc = correct / total

        # Save metrics
        history["train_loss"].append(train_loss/len(train_loader))
        history["val_loss"].append(val_loss/len(val_loader))
        history["train_acc"].append(train_acc)
        history["val_acc"].append(val_acc)

        print(f"Epoch {epoch+1}/{epochs} | "
              f"Train Loss: {history['train_loss'][-1]:.4f}, Acc: {train_acc:.4f} | "
              f"Val Loss: {history['val_loss'][-1]:.4f}, Acc: {val_acc:.4f}")

    return model, history

# ==============================
# Plot Training Curves
# ==============================
def plot_history(history):
    plt.figure(figsize=(10,4))
    plt.plot(history['train_loss'], label="Train Loss")
    plt.plot(history['val_loss'], label="Val Loss")
    plt.xlabel("Epochs"); plt.ylabel("Loss"); plt.title("Loss Curve")
    plt.legend(); plt.show()

    plt.figure(figsize=(10,4))
    plt.plot(history['train_acc'], label="Train Accuracy")
    plt.plot(history['val_acc'], label="Val Accuracy")
    plt.xlabel("Epochs"); plt.ylabel("Accuracy"); plt.title("Accuracy Curve")
    plt.legend(); plt.show()

# ==============================
# Evaluation
# ==============================
def evaluate_model(model, test_loader, device="cuda", class_names=None):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for X, y in test_loader:
            X, y = X.to(device), y.to(device)
            outputs = model(X)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=class_names))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted"); plt.ylabel("True"); plt.title("Confusion Matrix")
    plt.show()

from torchsummary import summary
num_classes = 5   # PTB-XL superclasses
class_names = ["NORM", "MI", "STTC", "CD", "HYP"]
model = ECG_CNN(input_channels=12, num_classes=num_classes)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Create model and move to device
model = ECG_CNN(input_channels=12, num_classes=5).to(device)

# Run summary on the same device
summary(model, (12, 1000), device=str(device))

num_classes = 5   # PTB-XL superclasses
class_names = ["NORM", "MI", "STTC", "CD", "HYP"]

model = ECG_CNN(input_channels=12, num_classes=num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# Train
model, history = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=80)
plot_history(history)

# Evaluate
evaluate_model(model, test_loader, class_names=class_names)

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ==============================
# CNN Feature Extractor
# ==============================
class ECG_CNN_FeatureExtractor(nn.Module):
    def __init__(self, input_channels=12, feature_dim=128):
        super(ECG_CNN_FeatureExtractor, self).__init__()

        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=7, stride=1, padding=3)
        self.bn1 = nn.BatchNorm1d(32)
        self.pool1 = nn.MaxPool1d(2)

        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)
        self.bn2 = nn.BatchNorm1d(64)
        self.pool2 = nn.MaxPool1d(2)

        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm1d(128)
        self.pool3 = nn.MaxPool1d(2)

        self.global_pool = nn.AdaptiveAvgPool1d(1)   # compress time dimension

    def forward(self, x):
        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))
        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))
        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))
        x = self.global_pool(x).squeeze(-1)   # (batch, features=128)
        return x


# ==============================
# Extract Features using CNN
# ==============================
def extract_features(model, dataloader, device="cuda"):
    model.eval()
    features, labels = [], []

    with torch.no_grad():
        for X, y in dataloader:
            X = X.to(device)
            feat = model(X)
            features.append(feat.cpu().numpy())
            labels.append(y.numpy())

    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    return features, labels


# ==============================
# Train SVM on CNN Features
# ==============================
def train_cnn_svm(train_loader, test_loader, class_names, device="cuda"):
    cnn_model = ECG_CNN_FeatureExtractor().to(device)

    # ---- Extract train/test features ----
    print("Extracting features from CNN...")
    X_train, y_train = extract_features(cnn_model, train_loader, device)
    X_test, y_test = extract_features(cnn_model, test_loader, device)

    # ---- Train SVM ----
    print("Training SVM...")
    svm = SVC(kernel="rbf", C=10, gamma="scale")
    svm.fit(X_train, y_train)

    # ---- Evaluate ----
    y_pred = svm.predict(X_test)

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=class_names))

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted"); plt.ylabel("True"); plt.title("Confusion Matrix (CNN+SVM)")
    plt.show()


# ==============================
# Example Usage
# ==============================
# Assuming you already defined:
# train_loader, test_loader, class_names = [...]
device = "cuda" if torch.cuda.is_available() else "cpu"

train_cnn_svm(train_loader, test_loader, class_names, device)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Create model and move to device
model_DL = train_cnn_svm(train_loader, test_loader, class_names, device)

# Run summary on the same device
summary(model_DL, (32, 12, 1000), device=str(device))

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ==============================
# ANN Model
# ==============================
import torch
import torch.nn as nn

class ECG_ANN(nn.Module):
    def __init__(self, input_size, num_classes=5):
        super(ECG_ANN, self).__init__()
        self.fc1 = nn.Linear(input_size, 512)   # was 1000 → should be 12000
        self.bn1 = nn.BatchNorm1d(512)
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.fc3 = nn.Linear(256, 128)
        self.bn3 = nn.BatchNorm1d(128)
        self.fc4 = nn.Linear(128, num_classes)

    def forward(self, x):
        # x shape: (batch, 12, 1000)
        x = x.view(x.size(0), -1)  # → (batch, 12000)
        x = torch.relu(self.bn1(self.fc1(x)))
        x = torch.relu(self.bn2(self.fc2(x)))
        x = torch.relu(self.bn3(self.fc3(x)))
        x = self.fc4(x)
        return x



# ==============================
# Training Function
# ==============================
def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20, device="cuda"):
    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}
    model.to(device)

    for epoch in range(epochs):
        model.train()
        train_loss, correct, total = 0, 0, 0

        for X, y in train_loader:
            X, y = X.to(device), y.to(device)

            optimizer.zero_grad()
            outputs = model(X)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == y).sum().item()
            total += y.size(0)

        train_acc = correct / total

        # ---- Validation ----
        model.eval()
        val_loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for X, y in val_loader:
                X, y = X.to(device), y.to(device)
                outputs = model(X)
                loss = criterion(outputs, y)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == y).sum().item()
                total += y.size(0)

        val_acc = correct / total

        # Save metrics
        history["train_loss"].append(train_loss/len(train_loader))
        history["val_loss"].append(val_loss/len(val_loader))
        history["train_acc"].append(train_acc)
        history["val_acc"].append(val_acc)

        print(f"Epoch {epoch+1}/{epochs} | "
              f"Train Loss: {history['train_loss'][-1]:.4f}, Acc: {train_acc:.4f} | "
              f"Val Loss: {history['val_loss'][-1]:.4f}, Acc: {val_acc:.4f}")

    return model, history

# ==============================
# Plot Curves
# ==============================
def plot_history(history):
    plt.figure(figsize=(10,4))
    plt.plot(history['train_loss'], label="Train Loss")
    plt.plot(history['val_loss'], label="Val Loss")
    plt.xlabel("Epochs"); plt.ylabel("Loss"); plt.title("Loss Curve")
    plt.legend(); plt.show()

    plt.figure(figsize=(10,4))
    plt.plot(history['train_acc'], label="Train Accuracy")
    plt.plot(history['val_acc'], label="Val Accuracy")
    plt.xlabel("Epochs"); plt.ylabel("Accuracy"); plt.title("Accuracy Curve")
    plt.legend(); plt.show()

# ==============================
# Evaluation
# ==============================
def evaluate_model(model, test_loader, device="cuda", class_names=None):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for X, y in test_loader:
            X, y = X.to(device), y.to(device)
            outputs = model(X)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=class_names))
    macro_auroc = roc_auc_score(y_true, y_pred, average="macro", multi_class="ovr")

# Step 3: Compute Macro AUPRC (one-vs-rest average precision)
    macro_auprc = average_precision_score(y_true, y_pred, average="macro")

    print(f"Macro AUROC: {macro_auroc:.4f}")
    print(f"Macro AUPRC: {macro_auprc:.4f}")
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted"); plt.ylabel("True"); plt.title("Confusion Matrix")
    plt.show()

# ==============================
# Usage Example
# ==============================
device = "cuda" if torch.cuda.is_available() else "cpu"

# suppose your ECG features are already flattened into vectors (not sequences/images)
# Suppose X_train.shape = (N, 1000)  → 1000 features per sample
# The input signal shape is (batch, channels, time_steps), which is (batch, 12, 1000)
# Flattened size is 12 * 1000 = 12000
input_size = 12 * 1000
num_classes = 5          # adjust to your labels

model = ECG_ANN(input_size, num_classes).to(device)


model = ECG_ANN(input_size, num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train model
model, history = train_model(model, train_loader, val_loader,
                             criterion, optimizer, epochs=30, device=device)

# Plot curves
plot_history(history)

# Evaluate
class_names = ['NORM','MI','STTC','CD','HYP']  # replace with your labels
evaluate_model(model, test_loader, device=device, class_names=class_names)

from sklearn.metrics import roc_auc_score, average_precision_score
evaluate_model(model, test_loader, device=device, class_names=class_names)

def evaluate_model(model, test_loader, device="cuda", class_names=None):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for X, y in test_loader:
            X, y = X.to(device), y.to(device)
            outputs = model(X)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=class_names))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d",
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted"); plt.ylabel("True"); plt.title("Confusion Matrix")
    plt.show()
class_names = ['NORM','MI','STTC','CD','HYP']
evaluate_model(model, train_loader, device=device, class_names=class_names)

summary(model, input_size=( 12, 1000))

import torch
import torch.nn as nn

class ECG_ANN_FeatureExtractor(nn.Module):
    def __init__(self, input_size=12000, embed_dim=128):
        super(ECG_ANN_FeatureExtractor, self).__init__()
        self.fc1 = nn.Linear(input_size, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.fc3 = nn.Linear(256, embed_dim)  # embedding output
        self.bn3 = nn.BatchNorm1d(embed_dim)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # flatten → (batch, 12000)
        x = torch.relu(self.bn1(self.fc1(x)))
        x = torch.relu(self.bn2(self.fc2(x)))
        x = torch.relu(self.bn3(self.fc3(x)))
        return x   # no softmax → we use as features

from sklearn.svm import SVC
from sklearn.metrics import classification_report
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ECG_ANN_FeatureExtractor().to(device)

# Assume model is trained (otherwise pre-train with CrossEntropy first)
model.eval()

X_train, y_train = [], []
X_test, y_test = [], []

with torch.no_grad():
    for data, labels in train_loader:
        data, labels = data.to(device), labels.cpu().numpy()
        feats = model(data).cpu().numpy()
        X_train.append(feats)
        y_train.append(labels)

    for data, labels in test_loader:
        data, labels = data.to(device), labels.cpu().numpy()
        feats = model(data).cpu().numpy()
        X_test.append(feats)
        y_test.append(labels)

X_train = np.vstack(X_train)
y_train = np.concatenate(y_train)
X_test = np.vstack(X_test)
y_test = np.concatenate(y_test)

svm = SVC(kernel="rbf", C=10, gamma="scale")
svm.fit(X_train, y_train)

y_pred = svm.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

rf = RandomForestClassifier(
    n_estimators=200,  # number of trees
    max_depth=20,     # limit tree depth
    random_state=42,
    n_jobs=-1         # use all cores
)

rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

print(classification_report(y_test, y_pred))

import torch
import torch.nn as nn

class ECG_LSTM(nn.Module):
    def __init__(self, input_size=12, hidden_size=128, num_layers=2, num_classes=5, dropout=0.3):
        super(ECG_LSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        self.fc = nn.Linear(hidden_size * 2, num_classes) # * 2 for bidirectional

    def forward(self, x):
        # Handle extra dimension
        if x.dim() == 4 and x.shape[-1] == 1:
            x = x.squeeze(-1)         # (B, 12, 1000, 1) -> (B, 12, 1000)

        # Rearrange to (batch, seq_len, input_size)
        x = x.permute(0, 2, 1)        # (B, 12, 1000) -> (B, 1000, 12)

        out, _ = self.lstm(x)         # pass through LSTM
        out = out[:, -1, :]           # last timestep
        out = self.fc(out)
        return out

from torchinfo import summary
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ECG_LSTM(input_size=12, hidden_size=128, num_layers=2, num_classes=5).to(device)

# Example input: 12-lead ECG with 1000 samples per lead
# We reshape it as (batch, seq_len=1000, input_size=12) if treating leads as features
# OR (batch, seq_len=1000, input_size=1) if processing one lead at a time
# The model expects input of shape (batch, channels, seq_len)
summary(model, input_size=(32, 12, 1000), device=str(device))

import torch.optim as optim
from sklearn.metrics import classification_report

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ECG_LSTM(input_size=12, hidden_size=128, num_layers=2, num_classes=5).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 2
train_losses, val_losses = [], []
train_accs, val_accs = [], []

for epoch in range(num_epochs):
    # ---- Training ----
    model.train()
    correct, total, epoch_loss = 0, 0, 0

    for data, labels in train_loader:
        data, labels = data.to(device), labels.to(device)

        # Reshape: (batch, 12, 1000) -> The model's forward method handles permutation

        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    train_losses.append(epoch_loss / len(train_loader))
    train_accs.append(correct / total)

    # ---- Validation ----
    model.eval()
    correct, total, val_loss = 0, 0, 0
    with torch.no_grad():
        for data, labels in val_loader:
            data, labels = data.to(device), labels.to(device)
            # No reshaping needed, model's forward handles permutation
            outputs = model(data)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    val_losses.append(val_loss / len(val_loader))
    val_accs.append(correct / total)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accs[-1]:.4f}, "
          f"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))

# Loss
plt.subplot(1,2,1)
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Val Loss")
plt.xlabel("Epochs"); plt.ylabel("Loss"); plt.title("LSTM Loss")
plt.legend()

# Accuracy
plt.subplot(1,2,2)
plt.plot(train_accs, label="Train Acc")
plt.plot(val_accs, label="Val Acc")
plt.xlabel("Epochs"); plt.ylabel("Accuracy"); plt.title("LSTM Accuracy")
plt.legend()

plt.show()

y_true, y_pred = [], []
model.eval()
with torch.no_grad():
    for data, labels in test_loader:
        data, labels = data.to(device), labels.to(device)
        data = data[:, 0, :].unsqueeze(-1)
        outputs = model(data)
        _, preds = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred))

# ==========================
# PTB-XL ECG Classification with Explainability (Grad-CAM)
# ==========================

#!pip install wfdb torch torchvision torchaudio scikit-learn matplotlib seaborn shap


import shap

# =====================
# Load PTB-XL Metadata
# =====================
'''
From google.colab import drive
drive.mount('/content/drive')

# Path to your PTB-XL zip file
zip_path = "/content/drive/MyDrive/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip"

# Unzip it inside /content/
import zipfile
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("/content/ptbxl")

print("✅ PTB-XL dataset extracted!")

'''
df

import matplotlib.pyplot as plt
data_path = "/content/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/"
sample_file = data_path + df.loc[0, 'filename_lr']  # low-resolution ECG (100 Hz)
record = wfdb.rdsamp(sample_file)

signal = record[0]   # numpy array, shape (1000, 12) -> 1000 samples, 12 leads
fields = record[1]   # metadata
print("Signal shape:", signal.shape)
print("Metadata:", fields)
plt.figure(figsize=(12, 6))
for i in range(12):
    plt.plot(signal[:, i] + i*2, label=f"Lead {i+1}")  # offset each lead
    plt.text(-1,2*i,fields['sig_name'][i])
plt.title("Sample PTB-XL ECG (100 Hz, 10s)")
plt.xlabel("Time (samples)")
plt.ylabel("Amplitude (mV, shifted)")
plt.legend(loc="upper right", ncol=4)
plt.show()

# ====== Install + Imports ======
#!pip install -q shap

import torch
import shap
import numpy as np
import matplotlib.pyplot as plt

# Disable cuDNN for RNNs as a workaround for SHAP compatibility
torch.backends.cudnn.enabled = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#model = model = BiLSTMClassifier(num_classes=len(label_map))

# ====== Build a small GPU background set (critical for speed/RAM) ======
# Take 8-16 training samples as background; more = heavier
with torch.no_grad():
    background = next(iter(train_loader))[0][:100].to(device)   # (16, 12, 1000)

# ====== Pick a small test batch to explain ======
with torch.no_grad():
    sample_data, sample_labels = next(iter(train_loader))
sample_data = sample_data[:100].to(device)                       # (8, 12, 1000)

# ====== SHAP GradientExplainer on GPU ======
# Works with differentiable PyTorch models; runs on GPU automatically.

# Set model to training mode for GradientExplainer to work
#model.train()
explainer = shap.GradientExplainer(model, background)

# Compute SHAP values for the batch (returns one array per class)
# Keep the batch small to avoid OOM.
shap_values = explainer.shap_values(sample_data)  # list length = num_classes

# Set model back to evaluation mode after computing SHAP values
model.eval()

# Each element has shape: (batch, 12, 1000)  <-- per-lead, per-timestep at input resolution

# ====== Pick target class per sample (predicted class) ======
with torch.no_grad():
    logits = model(sample_data)                     # (batch, num_classes)
pred = logits.argmax(dim=1)                         # (batch,)

for i in range(50):
  print(i, int(pred[i].item()))

import matplotlib.pyplot as plt
import numpy as np

# Choose which sample index to plot from the batch (0 to batch_size - 1)
sample_index_to_plot = 4
if sample_index_to_plot >= sample_data.shape[0]:
    print(f"Sample index {sample_index_to_plot} is out of bounds for a batch of size {sample_data.shape[0]}.")
else:
    i = sample_index_to_plot
    target_class = int(pred[i].item())

    print(f"Type of shap_values: {type(shap_values)}")
    if isinstance(shap_values, list):
        print(f"Length of shap_values: {len(shap_values)}")
        for j, val in enumerate(shap_values):
            if hasattr(val, 'shape'):
                print(f"Shape of shap_values[{j}]: {val.shape}")
            else:
                print(f"Type of shap_values[{j}]: {type(val)}")
    elif hasattr(shap_values, 'shape'):
         print(f"Shape of shap_values: {shap_values.shape}")


    # Hypothesis: shap_values is a single numpy array with shape (batch_size, leads, time_steps, num_classes)
    # Indexing to get SHAP values for sample i and target_class:
    try:
        sv = shap_values[i, :, :, target_class]
        print(f"Shape of sv: {sv.shape}")
    except IndexError as e:
         print(f"IndexError during indexing with [i, :, :, target_class]: {e}")
         raise


    sig = sample_data[i].cpu().numpy()   # shape: (12,1000)

    fig, axes = plt.subplots(6, 2, figsize=(16, 12), sharex=True)
    axes = axes.flatten()

    for lead in range(12):
        ax = axes[lead]

        # Normalize SHAP values for coloring
        importance = sv[lead]
        # Handle cases where importance is all the same value (e.g., all zeros)
        if importance.max() - importance.min() > 1e-8:
             importance_norm = (importance - importance.min()) / (importance.max() - importance.min())
        else:
             importance_norm = np.zeros_like(importance) # Or handle as appropriate

        # Plot ECG waveform
        ax.plot(sig[lead], color="black", linewidth=1.0)

        # Overlay SHAP heatmap
        # Use extent to correctly map data coordinates to image coordinates
        ax.imshow(importance_norm[np.newaxis, :],
                  aspect="auto",
                  cmap="seismic",    # red = positive impact, blue = negative
                  extent=[0, sig.shape[1], sig.min(), sig.max()], # xmin, xmax, ymin, ymax
                  alpha=0.9)

        ax.set_title(f" {fields['sig_name'][lead]}")
        ax.set_ylabel("Amplitude")
        ax.set_xlabel("Time (samples)")


    plt.suptitle(f"Sample {i} | Predicted Class = {target_class}", fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.97])
    plt.savefig(f"sample_{i}_pred_{target_class}.png",dpi=100)
    plt.show()
